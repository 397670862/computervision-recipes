{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2bd54cdb-aa39-48f9-8bb0-d602135c1b6c"
    }
   },
   "source": [
    "# Quickstart: Web Cam Action Recognition\n",
    "\n",
    "Action recognition is the increasingly popular computer vision task of determining specific actions in a given video.\n",
    "\n",
    "This notebook shows a simple example of loading a pretrained R(2+1)D model for action recognition and using a webcam stream to identify what actions are being performed.\n",
    "\n",
    "For more details about the underlying technology of action recognition, including finetuning, please see our [training introduction notebook](01_training_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite for Webcam example \n",
    "This notebook assumes you have **a webcam** connected to your machine.  We use the `ipywebrtc` module to show the webcam widget in the notebook. Currently, the widget works on **Chrome** and **Firefox**. For more details about the widget, please visit `ipywebrtc` [github](https://github.com/maartenbreddels/ipywebrtc) or [documentation](https://ipywebrtc.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "89d4b446-c421-488c-8967-8a38e538a9b2"
    }
   },
   "outputs": [],
   "source": [
    "# Regular Python libraries\n",
    "import sys\n",
    "from collections import deque #\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "from time import sleep, time\n",
    "from threading import Thread\n",
    "from IPython.display import Video\n",
    "\n",
    "# Third party tools\n",
    "import decord #\n",
    "import IPython.display #\n",
    "from ipywebrtc import CameraStream, ImageRecorder\n",
    "from ipywidgets import HBox, HTML, Layout, VBox, Widget, Label\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "# utils_cv\n",
    "sys.path.append(\"../../\")\n",
    "from utils_cv.action_recognition.data import KINETICS\n",
    "from utils_cv.action_recognition.dataset import DEFAULT_MEAN, DEFAULT_STD\n",
    "from utils_cv.action_recognition.model import VideoLearner\n",
    "from utils_cv.action_recognition.references import transforms_video as transforms\n",
    "from utils_cv.common.gpu import system_info, torch_device\n",
    "from utils_cv.common.data import data_path\n",
    "\n",
    "system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6db47566-8c49-44bb-aa28-c63bf3fb63cd"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf40a98f-414d-4827-8aa8-88ef6b9cc626"
    }
   },
   "source": [
    "## Load Pre-trained Model\n",
    "\n",
    "Load R(2+1)D 34-layer model pre-trained on IG65M or Kinetics400. There are also two versions of the model that we provide by default: an 8-frame model and 32-frame model based on the input clip length. As you'd expect, the 32-frame model is slower than 8-frame model. \n",
    "\n",
    "We'll start by setting some of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c456a2db-b030-46fc-8cd7-55c133feb354"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 400\n",
    "NUM_FRAMES = 8  # 8 or 32.\n",
    "IM_SCALE = 128  # resize then crop\n",
    "INPUT_SIZE = 112  # input clip size: 3 x NUM_FRAMES x 112 x 112\n",
    "\n",
    "# video sample to download\n",
    "url = \"https://cvbp.blob.core.windows.net/public/datasets/action_recognition/action_sample_lowRes.mp4\"\n",
    "# url = \"https://cvbp.blob.core.windows.net/public/datasets/action_recognition/action_sample.mp4\"  # highres\n",
    "\n",
    "# file path to save video sample\n",
    "video_fpath = data_path() / \"sample_video.mp4\"\n",
    "\n",
    "# prediction score threshold\n",
    "SCORE_THRESHOLD = 0.01\n",
    "AVERAGING_SIZE = 5  # Averaging 5 latest clips to make video-level prediction (or smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we just need to initialize our VideoLearner model and add the parameters we set above. By default, our VideoLearner will load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c6f4b85d-2f55-4d5a-8edb-5a9b9f8cc8b4"
    }
   },
   "outputs": [],
   "source": [
    "learner = VideoLearner(\n",
    "    base_model=\"kinetics\",\n",
    "    sample_length=NUM_FRAMES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8cec9405-d125-4681-9bdd-ec2184257de3"
    }
   },
   "source": [
    "## Prepare class names and prediction variables\n",
    "Since we use Kinetics400 model out of the box, we load its class names. The dataset consists of 400 human actions. For example, the first 20 labels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "eb1dfefe-dfe4-46e4-8482-9b2bc1d05b89"
    }
   },
   "outputs": [],
   "source": [
    "LABELS = KINETICS.class_names\n",
    "LABELS[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e7de627-d6db-422c-a406-91d15c93985c"
    }
   },
   "source": [
    "Among them, we will use 50 classes that we are interested in (i.e. the actions make sense to demonstrate in front of the webcam) and ignore other classes by filtering out from the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cb834095-db67-4b4b-87c3-a133a848a484"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_LABELS = [\n",
    "    \"assembling computer\",\n",
    "    \"applying cream\",\n",
    "    \"brushing teeth\",\n",
    "    \"clapping\",\n",
    "    \"cleaning floor\",\n",
    "    \"cleaning windows\",\n",
    "    \"drinking\",\n",
    "    \"eating burger\",\n",
    "    \"eating chips\",\n",
    "    \"eating doughnuts\",\n",
    "    \"eating hotdog\",\n",
    "    \"eating ice cream\",\n",
    "    \"fixing hair\",\n",
    "    \"hammer throw\",\n",
    "    \"high kick\",\n",
    "    \"jogging\",\n",
    "    \"laughing\",\n",
    "    \"mopping floor\",\n",
    "    \"moving furniture\",\n",
    "    \"opening bottle\",\n",
    "    \"plastering\",\n",
    "    \"punching bag\",\n",
    "    \"punching person (boxing)\",\n",
    "    \"pushing cart\",\n",
    "    \"reading book\",\n",
    "    \"reading newspaper\",\n",
    "    \"rock scissors paper\",\n",
    "    \"running on treadmill\",\n",
    "    \"shaking hands\",\n",
    "    \"shaking head\",\n",
    "    \"side kick\",\n",
    "    \"slapping\",\n",
    "    \"smoking\",\n",
    "    \"sneezing\",\n",
    "    \"spray painting\",\n",
    "    \"spraying\",\n",
    "    \"stretching arm\",\n",
    "    \"stretching leg\",\n",
    "    \"sweeping floor\",\n",
    "    \"swinging legs\",\n",
    "    \"texting\",\n",
    "    \"throwing axe\",\n",
    "    \"throwing ball\",\n",
    "    \"unboxing\",\n",
    "    \"unloading truck\",\n",
    "    \"using computer\",\n",
    "    \"using remote controller (not gaming)\",\n",
    "    \"welding\",\n",
    "    \"writing\",\n",
    "    \"yawning\",\n",
    "]\n",
    "len(TARGET_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3b3f708a-26d1-4a0c-bf1c-86fcf842751b"
    }
   },
   "source": [
    "# Action Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a video file:\n",
    "Here, we show how to use the model on a video file. We utilize threading so that the inference does not block the video preview. For this example, we'll use the following video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the video to our data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "open(video_fpath, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = str(data_path()/\"sample_video.mp4\")\n",
    "learner.predict(\n",
    "    video,\n",
    "    LABELS,\n",
    "    average_size=AVERAGING_SIZE,\n",
    "    score_threshold=SCORE_THRESHOLD,\n",
    "    target_labels=TARGET_LABELS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From your webcam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam settings\n",
    "w_cam = CameraStream(\n",
    "    constraints={\n",
    "        \"facing_mode\": \"user\",\n",
    "        \"audio\": False,\n",
    "        \"video\": {\"width\": 400, \"height\": 400},\n",
    "    },\n",
    "    layout=Layout(width=\"400px\"),\n",
    ")\n",
    "\n",
    "# Image recorder for taking a snapshot\n",
    "w_imrecorder = ImageRecorder(\n",
    "    format=\"jpg\", stream=w_cam, layout=Layout(padding=\"0 0 0 100px\")\n",
    ")\n",
    "\n",
    "# Text widget to show our classification results\n",
    "w_text = HTML(layout=Layout(padding=\"0 0 0 100px\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMS = Compose([\n",
    "    transforms.ToTensorVideo(),\n",
    "    transforms.ResizeVideo(IM_SCALE),\n",
    "    transforms.CenterCropVideo(INPUT_SIZE),\n",
    "    transforms.NormalizeVideo(DEFAULT_MEAN, DEFAULT_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_webcam_frames():\n",
    "    \"\"\" Predict activity by using a pretrained model\n",
    "    \"\"\"\n",
    "    global w_imrecorder, w_text, is_playing\n",
    "    global device, model\n",
    "\n",
    "    # Use deque for sliding window over frames\n",
    "    window = deque()\n",
    "    scores_cache = deque()\n",
    "    scores_sum = np.zeros(NUM_CLASSES)\n",
    "\n",
    "    while is_playing:\n",
    "        try:\n",
    "            # Get the image (RGBA) and convert to RGB\n",
    "            im = Image.open(io.BytesIO(w_imrecorder.image.value)).convert(\"RGB\")\n",
    "            window.append(np.array(im))\n",
    "\n",
    "            # update println func\n",
    "            def update_println(println):\n",
    "                w_text.value = println\n",
    "            \n",
    "            if len(window) == NUM_FRAMES:\n",
    "                learner.predict_frames(\n",
    "                    window,\n",
    "                    scores_cache,\n",
    "                    scores_sum,\n",
    "                    None,\n",
    "                    AVERAGING_SIZE,\n",
    "                    SCORE_THRESHOLD,\n",
    "                    LABELS,\n",
    "                    TARGET_LABELS,\n",
    "                    TRANSFORMS, \n",
    "                    update_println,\n",
    "                )\n",
    "            else:\n",
    "                w_text.value = \"Preparing...\"\n",
    "        except OSError:\n",
    "            # If im_recorder doesn't have valid image data, skip it.\n",
    "            pass\n",
    "        except BaseException as e:\n",
    "            w_text.value = \"Exception: \" + str(e)\n",
    "            break\n",
    "\n",
    "        # Taking the next snapshot programmatically\n",
    "        w_imrecorder.recording = True\n",
    "        sleep(0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_playing = False\n",
    "#  Once prediciton started, hide image recorder widget for faster fps\n",
    "def start(_):\n",
    "    global is_playing\n",
    "    # Make sure this get called only once\n",
    "    if not is_playing:\n",
    "        w_imrecorder.layout.display = \"none\"\n",
    "        is_playing = True\n",
    "        Thread(target=predict_webcam_frames).start()\n",
    "\n",
    "\n",
    "w_imrecorder.image.observe(start, \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBox([w_cam, w_imrecorder, w_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3b2047f3-87a7-47bd-a53c-2fa90540be99"
    }
   },
   "source": [
    "To start inference on webcam stream, click 'capture' button when the stream is started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "acf10288-9c35-45ec-8b20-cbb3782dead3"
    }
   },
   "source": [
    "## Stop Webcam and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a218abeb-4dd9-4030-9e54-550415ae8a89"
    }
   },
   "outputs": [],
   "source": [
    "is_playing = False\n",
    "Widget.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
