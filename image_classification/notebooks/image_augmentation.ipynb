{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep dive into image-augmentation:\n",
    "* what is image-augmentation and why it works\n",
    "* how it affects on different image classification tasks (when to use / not to use which)\n",
    "\n",
    "\n",
    "1. what image-augmentation is and why it works\n",
    "\n",
    "    a. FastAI's image augmentation transformations\n",
    "    \n",
    "    b. Examples ('not-very-useful or even bad' vs. 'good')\n",
    "    \n",
    "2. how it affects on different image classification tasks: examples\n",
    "\n",
    "    a. Hand-written number classification (MNIST)\n",
    "\n",
    "    b. Object classification (Fridge-object)\n",
    "    \n",
    "\n",
    "Further readings:\n",
    "* https://towardsdatascience.com/data-augmentation-experimentation-3e274504f04b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast.ai version = 1.0.48\n",
      "Fast.ai (Torch) is using GPU: GeForce GTX 1050\n",
      "Available / Total memory = 1980 / 2048 (MiB)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import accuracy\n",
    "import torch.cuda as cuda\n",
    "\n",
    "from utils_ic.datasets import unzip_url, Urls\n",
    "from utils_ic.fastai_utils import TrainMetricsRecorder\n",
    "from utils_ic.gpu_utils import which_processor\n",
    "from utils_ic.plot_utils import plot_pr_roc_curves, ResultsWidget\n",
    "\n",
    "\n",
    "print(f\"Fast.ai version = {fastai.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows your machine's GPUs (if has any) and which computing device fastai/torch is using. The output cells here show the run results on [Azure DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/) Standard NC6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "LEARNING_RATE = 1e-4\n",
    "IMAGE_SIZE    = 299\n",
    "BATCH_SIZE    = 16\n",
    "ARCHITECTURE  = models.resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "tmpdir = TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What image-augmentation is and why it works\n",
    "\n",
    "Some texts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastAI image augmentation transformations\n",
    "\n",
    "What we want to test from this notebook:\n",
    "* `get_transforms` (`do_flip` / `flip_vert`): True / True, True / False, False\n",
    "* `zoom_crop`\n",
    "* `rand_resize_crop`\n",
    "\n",
    "Note, each transform functions returns a tuple of two list of transforms: one for the training set and one for the validation set (the second list of transforms is limited to resizing the pictures). \n",
    "\n",
    "For the full list of transformation `fastai` supports, see [its API documents](https://docs.fast.ai/vision.transform.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep all the other parameters as defaults.\n",
    "tfms_options = {}\n",
    "tfms_options['flip'] = get_transforms(do_flip=True, flip_vert=True)\n",
    "tfms_options['flip_no_vert'] = get_transforms(do_flip=True, flip_vert=False)\n",
    "tfms_options['no_flip'] = get_transforms(do_flip=False)\n",
    "tfms_options['zoom_crop'] = zoom_crop(scale=(0.75,2), do_rand=True)\n",
    "tfms_options['rand_resize_crop'] = rand_resize_crop(224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of not-very-useful (or even bad) transformations vs. good transformations\n",
    "\n",
    "Again, this will be pretty much dependent on the problem we would like to solve.\n",
    "\n",
    "\n",
    "<img src=\"figs/augmentation_m_w.jpg\" alt=\"m and w\" width=\"200\"/>\n",
    "\n",
    "<center><i>You probablly do not want to vertical-flip letters</i></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/jumin/AppData/Local/Temp/tmp6bxgpv6e/mnist/mnist_tiny/labels.csv'),\n",
       " WindowsPath('C:/Users/jumin/AppData/Local/Temp/tmp6bxgpv6e/mnist/mnist_tiny/models'),\n",
       " WindowsPath('C:/Users/jumin/AppData/Local/Temp/tmp6bxgpv6e/mnist/mnist_tiny/test'),\n",
       " WindowsPath('C:/Users/jumin/AppData/Local/Temp/tmp6bxgpv6e/mnist/mnist_tiny/train'),\n",
       " WindowsPath('C:/Users/jumin/AppData/Local/Temp/tmp6bxgpv6e/mnist/mnist_tiny/valid')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNIST TINY\n",
    "mnist_path = untar_data(\n",
    "    URLs.MNIST_TINY,\n",
    "    os.path.join(tmpdir.name, 'mnist.tgz'),\n",
    "    os.path.join(tmpdir.name, \"mnist\")\n",
    ")\n",
    "mnist_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelLists;\n",
       "\n",
       "Train: LabelList (709 items)\n",
       "x: ImageList\n",
       "Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28)\n",
       "y: CategoryList\n",
       "3,3,3,3,3\n",
       "Path: C:\\Users\\jumin\\AppData\\Local\\Temp\\tmp6bxgpv6e\\mnist\\mnist_tiny;\n",
       "\n",
       "Valid: LabelList (699 items)\n",
       "x: ImageList\n",
       "Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28)\n",
       "y: CategoryList\n",
       "3,3,3,3,3\n",
       "Path: C:\\Users\\jumin\\AppData\\Local\\Temp\\tmp6bxgpv6e\\mnist\\mnist_tiny;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data = (ImageList.from_folder(mnist_path)\n",
    "    .split_by_folder()\n",
    "    .label_from_folder())\n",
    "mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABH4AAADvCAYAAACEwBPsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl81dW57/HngQAhIYQhGAgYIIAQAooMRUVBRQTqUGsRRQbRap162ttq9Vqt7Tna23vsZIdbtZ7anh7Faos91rmKooAo0Csqo8g8JECAQMKQgazzx95qpPtZCT8y7Kx83q+Xrxf5fbP2Xol77fXbT357P+qcEwAAAAAAAISnVVNPAAAAAAAAAA2Dwg8AAAAAAECgKPwAAAAAAAAEisIPAAAAAABAoCj8AAAAAAAABIrCDwAAAAAAQKAo/AREVQeq6nuqWqqqe1X1/vjxc1R1bVPPD0g2rBkAtVHVMaq6TlXLVPUyVZ2vqtfHs+mq+vemniMAABZV/YGqPt6I98femIQo/ITlDhGZ75zLEJG/fXLQObfAOTew6aYFJK0WvWZUdZOqXtDU8wCS3L+JyK+dcx2cc/9dM3DOPeGcu7CJ5gUAQNJhb0xOFH7C0ltEVjb1JIBmpEWuGVVNaeo5AM1Ii3yeAELGPohklgyPz2SYw4lo7vNvCBR+AqGqr4vIeSLya1UtE5G2NbJzVXVbja83qepdqrpKVfep6u9VNbUJpg00mYZeM5/chqrepqq7VLVQVa+tkWeq6h9VdbeqblbVe1TVfE5W1XaqWqKqQ2oc66aqh1X1pPjXF6vq8vj3va2qpx7zM9ypqh+IyEFVfVJEckXkufhbWO44nt8fkKzij/XbVfUDVd2vqk99sl5V9QZV/Tj+1s6/qWpOLbe1XkTy5LN10u6YfLaqLqzxtVPVb6jqBlUtVtUf+9Y10Fyp6pXxNfHJf+Xxt0Gae5uqtop/vTm+L/5RVTPjWZ/4+rlWVbfG99qbVHVUfC2XqOqv6zi3G1R1tcbexr1KVYfHjx+7D6aoan583iWqulJVL61xO39Q1YdV9dX4bb2pqr0b4NcJJHp83qOq62s8jr9c43tnq+pCVf1JfK1sVNXJNfK+8cdrqaq+KiJZdbj/T9bgV1V1i4i8Hj9+RvycskRV31fVc4+Zx4b4/WxU1ek15xf/9x3HPFdUquof4lmmqv5OY+fI21X1flVtXYe5ssYj4GQkEM6580VkgYh83TnXQUQqahkyXUQmikg/ETlFRO5p2BkCyaWR1kx3EckUkZ4i8lUR+X+q2jme/Sqe5YnIOBGZJSLXJrqR+HzLReQZEZlW4/BUEXnTObcrvuk9JiI3ikhXEXlERP6mn3+hOk1ELhKRTs65aSKyRUQuib+F5YE6/DxAczFVRCaJSF8ROVVEZqvq+SLyo3jWQ0Q2i8iffDfinOsnn18n5XW47y+LyEgRGS4iXxKR66L+EECycs49FV8THUQkR0Q2iMiT4t/bZsf/Oy+edxCRY4s5o0VkgIhcKSIPisjdInKBiBSIyFRVHeebl6peISI/iN9vRxG5VET21PiWT/dBEVEReU5E/i4iJ4nIv4jIE6pa863e00XkPom9cF4uIk/47h84QTUfn2tF5ByJrad/FZHHVbVHje8dHf+eLBF5QER+p6oaz+aIyD/i2X0ics1xzGGciOSLyERV7SkiL4jI/SLSRURuF5G5GvvDY7qI/FJEJsc/MuEsia2Rz3HOPVDjuSJfRHaLyNPx+D9FpEpE+ovI6SJyoYhc75scazw6Cj8t16+dc1udc3tF5Ify+ReTAP5ZlDVTKSL/5pyrdM69KCJlIjIw/teMK0XkLudcqXNuk4j8VERm1nJ7c46536vjx0REbhCRR5xz7zrnjjrn/lNEykXkjBrf/8v4z3C4DnMHmrNfOud2xNfrcyIyTGInd4855/5/vIBzl4icqap96vm+/905t9c5t0ViL1zZXxGs+NU8c0Rkvoj8h/j3tuki8jPn3AbnXJnE1uBV+vm3ZNznnDvinPu7iBwUkSedc7ucc9sl9sea02uZ0vUi8oBzbqmL+dg5t7lGXnMfPENixaf/65yrcM69LiLPy+fX7AvOubfizxl3S+w54+Tj+iUBdffp49M59+f4PlbtnHtKRNaJyBdqfO9m59yjzrmjEiug9BCRbFXNFZFRIvI951y5c+4tie2DdfUD59zB+BqZISIvOudejM/jVRFZJiJfjH9vtYgMUdX2zrlC55z5tmhVbS8i/y0iv3DOvaiq2SIyWUT+V/z+donIz0XkqlrmxxqPiMJPy7W1xr83S+yvNQBsUdbMHudcVY2vD0lsA8qS2FvLam5UmyV2ZZDP6yLSXlVHxy9FHSYif41nvUXktvilrCWqWiIiJx8zz60CtAxFNf79ybrLkRprLv7Cc4/Uvu6OF/srWpIfikiGiHxDat/bchJkKSKSXePYzhr/Ppzg6w61zOdkEVnvyWuuzxwR2eqcqzbm+7nvjz9n7BXWNBrOp483VZ2ln719v0REhsjn37L16T7nnDsU/+cne90+59zBGt9bc93VeQ4SO7e84phzy7NFpEf89q8UkZtEpFBVX1DVQZ7b/Z2IrHXO/XuN224TH/vJbT8isStzfFjjEfGhRy1XzUpmrojsaKqJAM1Efa6ZYoldDdRbRFbVuM3tvkHOuWpVfVpif6nYKSLPO+dK4/FWEfmhc+6Hvpuo5WsgZDsktuZERCR+mXpXqWXdRXCyfPZh0OyvCJaqXiWx/WiUc65SVWvb2z63BuNZlcT2s171NK2tEntLtqXmvrdDRE5W1VY1XhjmishHNb7n071fVTtI7O0urGk0FCciEv/j3qMiMl5EFjvnjqrqcom9dak2hSLSWVXTaxR/cqXu53w1v2+riPyXc+6GhN/o3Csi8kr8ap7743M+59jvU9X/LSIDJVY0qnnb5SKSdcwfSWvDGo+IK35arltVtZeqdhGR74rIU009ISDJ1duaiV+W+7SI/FBVM+Ib/LdF5PE6DJ8jsb+wTJfP3uYlEttsb4pfDaSqmq6qF6lqhue2dkrscxaAlmCOiFyrqsPin331f0Tk3fjbUerTd1S1c/xS8W8K+ysCpKqnS+zzfC5zzu0WqdPe9qSIfEtjHzzbQWJr8KnjfNFXm/8QkdtVdUR8L+zv+bDWdyX2drI7VLVN/ENrL5HPf/bXF1X1bFVtK7HPAXnXOcfVs2ho6RIrYOwWEdFYc5Ah3hFx8bc9LRORf1XVtqp6tsQe11E8LiKXqOpEVW2tqqkaa17SS1WzVfXS+B9RyiX2cQZHj70BjX3o9Dck9lzx6UcNOOcKJfbZOz9V1Y4a+/D3frV9jpewxiOj8NNyzZHYYtsQ/+/+pp0OkPTqe838i8Q2ow0isjB++4/VNsg598kmliMiL9U4vkxin/PzaxHZJyIfS+xDNH1+JCL3xC+xvf34fwSg+XDOzROR74nIXIn9RbSf1P5ZAlE8K7EP1VwusQ/F/F0D3AfQ1L4kIp1FZKF+1q3nJfHvbY+JyH+JyFsislFEjsS/v9445/4ssbefzRGRUol9pkgX43srJPbBsJMldiXub0RklnNuTY1vmyMi35fY2z9GSOyPLkCDcs6tktjnYy2W2B/phorIouO4iasl9uHPeyX2+P1jxHlsldha/67EilBbReQ7EqshtBKR2yR2dcxeiX0o9C0JbuZKEekmIqtrPFc8HM9mSeztoaskdu76F4l9VpFvTqzxiNQ5rvRvaVR1k4hc75x7rannAjQHrBkAdaGqTkQGOOc+buq5ADgxGms5vc05R+dbIEAtbY1zxQ8AAAAAAECgKPwAQESq+t0al62WHXO5e9TbfNi4zYdrHw3geKnqOcaaK2vquQH4Z+yTQDSqOt1YO2Yb9qbAGm8YvNULAAAAAAAgUFzxAwAAAAAAEKiUxryz+IceAi2Wc06beg6JsDbR0rE2geTE2gSSE2sTSE7W2uSKHwAAAAAAgEBR+AEAAAAAAAgUhR8AAAAAAIBAUfgBAAAAAAAIFIUfAAAAAACAQFH4AQAAAAAACFSjtnMHAAAAAADhUU3YSbxZcc419RQaBFf8AAAAAAAABIrCDwAAAAAAQKAo/AAAAAAAAASKwg8AAAAAAECgKPwAAAAAAAAEisIPAAAAAABAoGjnDgBNLDc318zy8/PNrKCgwMyGDh0a6f4WLFhgZo899piZ7dixw8yqqqrMDAAAAE2jd+/eZtamTRszGzFiRMLjd999tzmmXbt2dZ9YA1u0aJGZ+X6GwsLChphOo+CKHwAAAAAAgEBR+AEAAAAAAAgUhR8AAAAAAIBAUfgBAAAAAAAIFIUfAAAAAACAQFH4AQAAAAAACBTt3AG0WK1a2bXvzp07R8ouuugiM/v5z3+e8PjRo0fNMVE55yKNO/vss81s+PDhZnbvvfea2erVq82svLy8bhMDTpCqmllBQUHC44MGDTLHDBw40Mz69etnZn369DGzjIwMM3v11VfN7P777zezw4cPm1nU5wkAQHLJzc01s/PPP9/MJk6caGa+du7WPufbN33n3Y0tMzPTzLZu3WpmDz/8sJkVFRUlPJ4se23y/PYBAAAAAABQryj8AAAAAAAABIrCDwAAAAAAQKAo/AAAAAAAAASKwg8AAAAAAECgKPwAAAAAAAAEinbuSDppaWlmdsopp5jZkCFDzKysrCzh8UWLFpljdu/ebWZoPjp27GhmI0aMMLPLL7/czHytKseNG2dmVtv2ZGnzKOJveT1p0iQzmzdvnpnt2LHDzHbt2lW3iQFxvn2gc+fOZjZz5kwzu+WWWxIeT6a16dvjrHb0IvbPJiKyffv2E5oTUFfZ2dlmlpqaamb9+/dPeHzGjBnmmF69etV9Yg1s8+bNZjZnzhwzmz9/vplVV1efyJQQqKlTp5rZlClTzMy3p/rar6enpyc87juPTCZdu3Y1syuvvNLM3njjDTPbs2dPwuMVFRV1n1gD4oofAAAAAACAQFH4AQAAAAAACBSFHwAAAAAAgEBR+AEAAAAAAAgUhR8AAAAAAIBA0dULTcL3ie95eXlm9rWvfc3MLr30UjMrKSlJePxHP/qROebJJ580MzoqNB8XXXSRmd18881mdtppp5lZu3btzGz//v1mVlRUlPD4zp07zTG+rleZmZlmNmzYMDPLysoyM9/a9M2zdevWZnbw4EEzQ8vVu3dvM7vuuuvMbPbs2Wbm6+KXkZFhZsnUvcvStm1bM7v44ovN7IknnjCzuXPnmpnVhRAtW8+ePc1s7NixZnbBBReYmW8vszp05efnm2OsbkNNobS01Mx8z1e+80yrI21lZWXdJ4bg5Obmmtnpp59uZr7zt8OHD5vZ+++/n/C4bz+Nutd269bNzHJycszM97P5znetboIi/g6FvvtLBlzxAwAAAAAAECgKPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBovADAAAAAAAQKAo/AAAAAAAAgaKdO5pEhw4dzGz06NFmNmnSJDPr3r27mXXq1Cnh8RkzZphjlixZYmbr1q0zMyQXX8toXzvY9u3bm5nVll1E5MEHHzSz9evXJzy+Z88ec0xxcbGZlZeXm9nUqVPN7LbbbjMza62I+NvSVlRUmNmhQ4fMDC3XrbfeambXX3+9mflaP/taIPvW2d69exMe9z3mDxw4YGadO3c2s1NOOcXM0tLSzMzHN0/f76tNmzZmRjt3JDJz5kwzmzJlipnl5eWZme9x2K5du4THW7VKnr9d+1pUZ2RkmNn5559vZhs3bjSzNWvWJDzuOzdB+ObNm2dmBQUFZpaSYpcD9u3bZ2YvvPBCwuNVVVXmmKh85+uzZ882M99e7GOdE4iI7N+/38x85yDJIHmeNQEAAAAAAFCvKPwAAAAAAAAEisIPAAAAAABAoCj8AAAAAAAABIrCDwAAAAAAQKAo/AAAAAAAAASKdu4tiK+Fuq+Vn68Vs6+Fs0/Xrl3NrFevXpFu09fO1mqnec4555hjbr75ZjO74447zKwh2hgiumeeecbM3nvvPTPbtWuXmW3fvt3MfK0vKysrzSyK3NxcM/Otd1+reh/fbS5btszMfG13aRndcn3wwQdm5mtLXF5ebmabNm0ys5deesnM/vGPfyQ8vnv3bnOMLzty5IiZ3XfffWY2ffp0M2vbtq2ZFRcXR5qL73cJJOJrXd67d28z69ixo5mVlZWZ2YIFCxIeP3DggDnGxzf/7t27m9mAAQPMLGrL6E6dOplZdna2mUXdwxG2559/3syWLFkS6TZ9r2msfce3xqLKysoys/Hjx5tZZmammbVu3drMli5damarVq0ys2TfU7niBwAAAAAAIFAUfgAAAAAAAAJF4QcAAAAAACBQFH4AAAAAAAACReEHAAAAAAAgUBR+AAAAAAAAAtUi27mnpqaaWY8ePczM13bR1/p5x44dZtYQLe/atWuX8Pi5555rjunWrZuZ+Vpef/jhh2bma9O8bds2M/vVr35lZu+8846ZffOb3zSzCy+8MOFx63clIjJr1iwzu/vuu82Mdu7JZd26dWa2ceNGM/M9fqurq09oTsfD18Jy7NixZjZmzBgz8z3ufXzPV4cOHYp0m2i5nnrqKTN78cUXzczXytjXSrW0tDTSuCgGDhxoZl26dDGzlJRop2UVFRVm5nsObIhzEITt2WefNTNfi/W0tDQzO3z4sJmtXLky4XFfC/iounbtamY33XSTmU2cONHMfPttYWGhma1evdrM9u7da2ZouXznrb7HWnPwhS98wcx858mqGun+du/ebWa+56tkxxU/AAAAAAAAgaLwAwAAAAAAECgKPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBovADAAAAAAAQqBbZzj0vL8/MZs6caWYjR440s/nz55vZz372MzNriJZw+fn5CY/ffvvt5phBgwaZ2eOPP25mDz74oJn5Wrb7Wp772lRu377dzI4cOWJmFl+bv9TUVDNr1YqaaXPha1dcWVnZiDMR6dChQ8Ljw4YNM8dcfvnlZjZp0iQz8z3P+drRr1mzxsxeeuklM/O1k/bdH1ou3/prDu2Ku3fvbmazZs0ys+HDh5uZb2/xrSPfXvbBBx+YGXC81q5da2Zbtmwxs6iPbevcriH2Fd++efDgwXq/v6KiIjP7+OOPzezAgQP1PhegqZ133nlmNnXqVDPr1KmTmUVt5+577VtSUhLpNpMBr14BAAAAAAACReEHAAAAAAAgUBR+AAAAAAAAAkXhBwAAAAAAIFAUfgAAAAAAAAJF4QcAAAAAACBQQbdzb926dcLjZ5xxhjlmwoQJZuZruZydnW1mTz75pJlt2LDBzKK65JJLEh73tans1q2bmY0YMcLMfL8TXzv3huBr223xtZVftGiRmdGeumVLT083s5ycHDMbPHhwwuNXX321Oebcc881s8zMTDPztc71tcr2PV/NnTvXzHzrPcraBBpTRkZGwuNXXXWVOebrX/+6meXn55uZdW4i4t9b1q9fb2Yvv/yymUVtZwsk4ns+P3ToUCPOJJo+ffqY2eTJk83M2r9FRNq0aRNpLqtWrTKzDz/80MzYU5HMfK+zv/SlL5nZ6NGjzWzAgAFmlpqaama+/W/r1q1m5jun9b12THZc8QMAAAAAABAoCj8AAAAAAACBovADAAAAAAAQKAo/AAAAAAAAgaLwAwAAAAAAECgKPwAAAAAAAIEKup17bm5uwuNnnXWWOaZ///4NNZ1Gc/rppyc83rFjx0i352s5v2bNmki3mSyOHj1qZgsXLow0Dsmlffv2ZjZw4EAzmzRpkpmNGzcu0v1Za/DUU081x/jaO0flW9NFRUVmtnPnTjM7cuTICc0JqKldu3ZmlpeXZ2a+tdSrVy8zs1o8+1rPdu/e3cx8Ldt9SktLzeyRRx4xs6efftrMWJsIke95wNdOesyYMWY2atSoSPfXqpX9d/QDBw6Y2cqVK81s06ZNZgYk0qlTJzM77bTTzMz32tfXDt1y4YUXmpnvNXhWVpaZ+fbUiooKM5s3b56Zvfnmm2ZWXFxsZs45M0t2XPEDAAAAAAAQKAo/AAAAAAAAgaLwAwAAAAAAECgKPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBCrqde0pK4h/P1xLO17ausrLSzHztGn3jmoOqqioza+yfLWoLPav9+vbt280xc+fONTPf7wSNLzU11cx8bV1vvPFGM/O1fO3WrZuZ+dq6RhGllWZthg8fbmazZs0ys7Vr15rZO++8c0JzQsvjW7df/OIXzeyGG24ws6FDh5pZmzZtjjvLzMw0xzSEsrIyMysvL480ztr/gCgyMjLMbNCgQWbma4fuW5sW395+7rnnmlnv3r3NzPec5Dvv+/DDD81s4cKFZrZ48eJI94eWy9ey/YorrjCzCRMmmFlBQYGZRTmn7devX73eXm127NhhZo8++qiZvf3222a2b9++E5pTsuKKHwAAAAAAgEBR+AEAAAAAAAgUhR8AAAAAAIBAUfgBAAAAAAAIFIUfAAAAAACAQFH4AQAAAAAACFTQ7dwHDx6c8Hjfvn3NMWlpaWbma/39xhtvmFlhYaGZNQe+FupR26v7tG3b1sx8bTjz8/PNzJrnoUOHzDHr1q0zs+rqajND42vfvr2Z5ebmmtno0aPNrHv37pHmcvjwYTPbsGFDwuNbt241x/gyX6v3IUOGmJnv5/a13PW18U1PTzezgwcPmhnC5muPfPHFF5vZww8/bGadO3c+oTkdD98ai8q3b/bs2dPMpkyZYmZLliwxs2XLltVtYkBcly5dzGzatGlmdv7555uZr9W777zP0qNHDzPzPe/41p9vvRcXF5vZn//8ZzP705/+ZGa+1xWcZyKRXr16mdkdd9xhZn369DGz+t7nfLfXEK8b9+zZY2YrVqwws927d5tZqOuPK34AAAAAAAACReEHAAAAAAAgUBR+AAAAAAAAAkXhBwAAAAAAIFAUfgAAAAAAAAIVdFevyy+/POHx4cOHm2NatbJrYUePHjUzX3eoqqoqM4vK+tlE7G4+vo5lvk9gj5pFlZOTY2Zjx441M98n1lv/79asWWOOqaysNDMkl4qKCjNbuXKlmT300ENm5usg9/rrr5vZ5s2bzWz//v0Jj/sea74sMzPTzMaMGWNmvq5evq5CJ598spn5On7R1avl8nXwKC0tNTPfvllWVmZmvi4dRUVFx535unJu27bNzLKyssxs0qRJZmZ1IxXxdzHq1KmTmQHHKzs728xmz55tZkOHDjWzlJT6fckRtXNQ1PNW321ae7uIvxtYeXl5pLmg5fK9Fj1w4ICZ+TrORn29Y60J32tp3zrydffzde8dOXKkmZ111llm5tvDfb+v5owrfgAAAAAAAAJF4QcAAAAAACBQFH4AAAAAAAACReEHAAAAAAAgUBR+AAAAAAAAAkXhBwAAAAAAIFBBt3Pv0qVLwuPp6enmGF+buahZQ5g1a5aZWW04o7bXawj9+/c3M9/PdsUVV5iZr1Wo1ar3mWeeMceg+fC1C1++fLmZbdq0ycx8bc23bNliZocOHTIzq2VmdXW1OcZnz549ZuZr7+xrPetrEe9r5+57XkXL5WtX/Oabb5rZhAkTzOziiy82s6VLl5qZb91arVsrKirMMb6fzdd6dseOHWb2k5/8xMxOOukkM6OdO+pTSUmJmc2bN8/Mjhw5Yma+fce3lqzz06ht2Xv06GFmAwcONLOcnBwzKygoMLOsrCwz87XfBhL56KOPzOyaa64xM9/rpwULFphZY7Y1/8pXvmJm119/vZmlpaWZWWO/vk12XPEDAAAAAAAQKAo/AAAAAAAAgaLwAwAAAAAAECgKPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBCrqduyWZWru1adPGzMaPH29mw4YNM7PU1NQTmtOx8vLyzOzUU081M197valTp5rZtGnTzMzXTtrXDtRq6e1rS4owVFVVmVlxcXGkrDH5Wtb6WjifccYZZlZWVmZmvnbuubm5ZtahQwczAxLxtX5etWqVma1YscLMkmV/963bdu3aRbpN33rv2bOnmfnWtK/FNlquwsJCM3vggQfMbNSoUWa2ZcsWM/PtSfW9pn3nrXfddZeZnXnmmfU6DyCKo0ePmpmv1fsjjzxiZgcOHIh0f/Vtz549ZuZ7beh7vYnP44ofAAAAAACAQFH4AQAAAAAACBSFHwAAAAAAgEBR+AEAAAAAAAgUhR8AAAAAAIBAUfgBAAAAAAAIVNDt3H3tVKPwtUnv0aOHmWVlZZnZiBEjzGz69Olm1qVLFzNr3bq1mUXha3155513mtnBgwfNrG/fvmbWu3dvM6usrDSz1atXm9lDDz2U8LivdSCQDHzPO2PHjjWzWbNmmVl2draZVVdXm1lOTo6Z+VpUp6TYW01VVZWZoeVqzBayUbVv397Mxo8fb2a+telbf61a2X+r69Onj5nRzh31ae/evWb2yiuvNOJMovE9t+zatcvM6rutPFDfKioqzGzHjh2NOJNoOnbsaGa+/Y+1WXdc8QMAAAAAABAoCj8AAAAAAACBovADAAAAAAAQKAo/AAAAAAAAgaLwAwAAAAAAECgKPwAAAAAAAIGinftx8LVEPeecc8zse9/7npmdcsopZjZkyBAz87VOttoj+1qXZ2RkmFnXrl0jZVH52vLt27fPzF5++WUze/XVV09oTmh6vsdov379zGzNmjVmVl5ebmYN0R7Sakfpa2FZUFBgZjNnzjSzvn37mpnvudHXztbXxresrMzMWrdubWa0c2/+fI+n3r17m5nv8VRaWmpmjdm61bfXDhw40Myuu+46M/P9Tnwta31tqE866SQzS01NNTOEzbc2fedvvnMt3+MwWfh+tokTJ5pZXl6emdX3awq0bL7net/jcNOmTWbWHM6nfOe01157rZmlpaU1xHRaHK74AQAAAAAACBSFHwAAAAAAgEBR+AEAAAAAAAgUhR8AAAAAAIBAUfgBAAAAAAAIFIUfAAAAAACAQAXdzr2+tW/f3sx87el8raZ9rWJTUqL973n//fcTHp8/f745pnv37mbmaxPra6sbNfO1Ct2yZYuZPfPMM2ZZGCt5AAAKT0lEQVTWHFoctiRt27ZNePzkk082x4wdO9bMJkyYYGb33nuvmW3evNnMKisrzczXVtLXmt1qMTtkyBBzjO9nGzp0qJn5nlt8LdvfeOMNM5szZ46Zbdu2zczKy8vNDM1D69atzWzAgAFmds0115jZvHnzzGzhwoVmduTIETOLymqtm5+fb47xtWwfPXq0mfnW5oEDB8xs+fLlZvb888+b2fr1680MzV+nTp3MrG/fvmZ2wQUXmNkf/vAHMysuLjYz37ldVNZzT2Zmpjnmy1/+spndeuutZjZ48GAzq66ujpQ1xO8EzUO3bt3MzHfe51ubP/7xj82spKSkbhOrJ23atEl43PeacubMmWZ2xRVXmFlqaqqZ+daf7zVlS8QVPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBovADAAAAAAAQKAo/AAAAAAAAgaLwAwAAAAAAEKig27mXlpYmPO5rBetrF+fja3Wbnp4e6TZ9ysrKzOzZZ59NePyxxx4zx2RkZJiZ1YK6NlHbufva8vlaFa5Zs6ZuE0OTy8rKSnh88uTJ5hhfC8hhw4aZ2dtvv21mO3fuNLP27dubma9luy/r3LlzwuNjxowxx/jay6ak2E/hmzZtMrP58+eb2UMPPWRmGzduNDPr+RbNi9WedezYseaYe+65x8xGjRplZr69xdfy3JpjbbfZpUsXM7OekyZNmmSOGTRokJn5zgkOHjxoZkuXLjUz3x7+5ptvmhntbMNgPe4vu+wyc8wNN9xgZiNHjjSz5cuXm9m6devMrCEea9nZ2QmPn3nmmeaY6667zszy8/PNrKKiwsyKiorMbMuWLWZ26NAhM0MYWrVKfB3FhAkTzDG33HKLmY0ePdrM/vrXv5rZ9u3bzcz3uiuqnJychMfHjx9vjpk9e7aZZWZmmplvHRUWFppZcXGxmbXEvZErfgAAAAAAAAJF4QcAAAAAACBQFH4AAAAAAAACReEHAAAAAAAgUBR+AAAAAAAAAkXhBwAAAAAAIFBBt3NfvHhxwuN5eXnmGF+bR19756qqKjPztTkuLy83M19bPl/ryHnz5iU87mtpt23bNjMD6pu1Br/61a+aYwoKCiLdl6/V7ZEjR8wsPT3dzKK2c7daVXbq1Mkc42v9vHr1ajNbsGCBmVnPESIi69evjzSXhmgVisZntYz+zne+Y44ZMWKEmfn2zXHjxpmZb5/2tXrv0KGDmfnWdFpaWsLjvhbwKSn2KZSv9ayvVfbjjz9uZn/5y1/MzHcOgjC0bds24fFvfetb5pjBgwebmaqa2Y033mhm+/fvN7OG2Aesvd93TuB7jvDtY++8846Z+dpov/TSS2a2c+dOM0MYrL3gzjvvNMf4Hr++deQ7T/a93mwI1h4+bNgwc0yrVvY1J7726kuWLDGz3//+92b21ltvmVlFRYWZhYorfgAAAAAAAAJF4QcAAAAAACBQFH4AAAAAAAACReEHAAAAAAAgUBR+AAAAAAAAAkXhBwAAAAAAIFBBt3P/zW9+k/B4WVmZOWbGjBlmNmDAADPbt2+fmS1dutTMfG3Uf/GLX5iZrw314cOHEx6vrq42xwCNyWrZGPUx6mt9OXbs2Ei3GZVvLoWFhQmPFxUVmWM+/vhjM3vttdfMzNdedsuWLWbma6eJ8FntTbOysswxVgv42vhatvfv3z/SbUZl7ZvWcRGRDRs2mJlvbT766KNmtnLlSjNDy2btLda+IiLSs2dPM8vMzDSzyy67rO4TqweVlZVmZp0XlJeXm2M2b95sZm+//baZ/fa3vzWz9957z8w4v0ZjmTZtmpk19uMwNTU14fHWrVubY/bu3Wtm8+fPN7Pvf//7ZuY7p/W9Xm6JuOIHAAAAAAAgUBR+AAAAAAAAAkXhBwAAAAAAIFAUfgAAAAAAAAJF4QcAAAAAACBQFH4AAAAAAAACpb7Ww/V+Z6qNd2ce6enpZta3b18z87XFLCkpMbPly5ebma8dJcLjnNOmnkMijb02+/Tpk/D4pZdeao4ZM2aMmflatnft2rXO86qpqqrKzHbv3m1mmzZtMrNFixYlPL527VpzjK9l9NatW83M1+KX551/xtr0++lPf2pmZ511lpkNGDDAzDp37nxCc0rEt273799vZitWrDiu4yL+vX3p0qVmtm7dOjOj9ew/Y23GtGqV+G+1BQUF5pgZM2ZEynz7pq9Vs+/x68t8+9y+ffsSHl+1apU55rnnnjOzZcuWmVlpaamZ4Z+xNmOstXn11VebY3zZhRdeeMJzOh6+WkBRUZGZWWt65cqV5hjf2nz33XfNzHeeXFlZaWYtlbU2ueIHAAAAAAAgUBR+AAAAAAAAAkXhBwAAAAAAIFAUfgAAAAAAAAJF4QcAAAAAACBQLbKrl4+q/QH1jfm7QpjogBCTmpqa8HiPHj3MMQMHDjQzX1chX8evYcOGmdnBgwfN7JVXXjGz+fPnm9nixYsTHvd156LLT+Ngbfr179/fzHxrbNy4cWY2cuRIM+vXr5+Z+boKffDBB2b2+uuvm9ncuXOP+/YOHTpkZqg/rM3o2rZta2ZTpkwxs29/+9tmlp2dbWYLFiwws7feesvMXnvtNTPbvHlzwuN08ml6rE2/bt26mZnv/POaa64xs0suucTM0tLSzMy3l61Zs8bMFi5caGbFxcUJj/s6gfnuy7o9EV6DHy+6egEAAAAAALQwFH4AAAAAAAACReEHAAAAAAAgUBR+AAAAAAAAAkXhBwAAAAAAIFAUfgAAAAAAAAJFO3egEdH6MrqUlBQzy8/PN7MhQ4aYWWZmppnt2rXLzNatW2dmvjaWJSUlCY/TlrbpsTaj87VXHzx4sJmdd955ZjZo0CAz27lzp5ktXrzYzFavXm1m1rplbTY91iaQnFib0fnOaUeNGmVmkydPNjNfO/cVK1aY2UcffWRmK1euNLP9+/ebGZoW7dwBAAAAAABaGAo/AAAAAAAAgaLwAwAAAAAAECgKPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBop070IhofQkkJ9YmkJxYm0ByYm0CyYl27gAAAAAAAC0MhR8AAAAAAIBAUfgBAAAAAAAIFIUfAAAAAACAQFH4AQAAAAAACBSFHwAAAAAAgEBR+AEAAAAAAAgUhR8AAAAAAIBAUfgBAAAAAAAIFIUfAAAAAACAQFH4AQAAAAAACBSFHwAAAAAAgEBR+AEAAAAAAAiUOueaeg4AAAAAAABoAFzxAwAAAAAAECgKPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBovADAAAAAAAQKAo/AAAAAAAAgaLwAwAAAAAAECgKPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBovADAAAAAAAQKAo/AAAAAAAAgaLwAwAAAAAAECgKPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBovADAAAAAAAQKAo/AAAAAAAAgaLwAwAAAAAAECgKPwAAAAAAAIGi8AMAAAAAABAoCj8AAAAAAACBovADAAAAAAAQqP8Bm4GqutNdc4kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_ex(im, tfms_ops):\n",
    "    _, axes = plt.subplots(1, len(tfms_ops), figsize=(4*len(tfms_ops), 4))\n",
    "    for i, (name, tfms) in enumerate(tfms_ops.items()):\n",
    "        im.apply_tfms(tfms[0]).show(ax=axes[i], title=name)  # tfms[0]: tfms for the training set\n",
    "\n",
    "plot_ex(mnist_data.train.x[0], tfms_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, models.resnet18, metrics=[accuracy])\n",
    "learn.fit_one_cycle(1,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, metric = learn.validate(learn.data.valid_dl, metrics=[accuracy])\n",
    "print(f'Accuracy on validation set: {100*float(metric):3.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use a toy dataset called *Fridge Objects*, which consists of 134 images of can, carton, milk bottle and water bottle photos taken with different backgrounds. With our helper function, the data set will be downloaded and unzip to `image_classification/data`.\n",
    "\n",
    "Let's set that directory to our `path` variable, which we'll use throughout the notebook, and checkout what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(DATA_PATH)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that we have four different folders inside:\n",
    "- `/water_bottle`\n",
    "- `/milk_bottle`\n",
    "- `/carton`\n",
    "- `/can`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use fastai, we want to create `ImageDataBunch` so that the library can easily use multiple images (mini-batches) during training time. We create an ImageDataBunch by using fastai's [data_block apis](https://docs.fast.ai/data_block.html).\n",
    "\n",
    "For training and validation, we randomly split the data by 8:2, where 80% of the data is for training and the rest for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_folder(path) \n",
    "        .split_by_rand_pct(valid_pct=0.2, seed=10) \n",
    "        .label_from_folder() \n",
    "        .transform(size=IMAGE_SIZE) \n",
    "        .databunch(bs=BATCH_SIZE) \n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at our data using the databunch we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figsize=(15,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see all available classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of classes: {data.c}')\n",
    "print(data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how many images we have in our training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.batch_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we don't use test set. You can add it by using [add_test](https://docs.fast.ai/data_block.html#LabelLists.add_test). Please note that in the **fastai** framework, test datasets have no labels - this is the unknown data to be predicted. If you want to validate your model on a test dataset with labels, you probably need to use it as a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model, we use a convolutional neural network (CNN). Specifically, we'll use **ResNet50** architecture. You can find more details about ResNet from [here](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "When training a model, there are many hypter parameters to select, such as the learning rate, the model architecture, layers to tune, and many more. With fastai, we can use the `create_cnn` function that allows us to specify the model architecture and performance indicator (metric). At this point, we already benefit from transfer learning since we download the parameters used to train on [ImageNet](http://www.image-net.org/).\n",
    "\n",
    "Note, we use a custom callback `TrainMetricsRecorder` to track the accuracy on the training set during training, since fast.ai's default [recorder class](https://docs.fast.ai/basic_train.html#Recorder) only supports tracking accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(\n",
    "    data,\n",
    "    ARCHITECTURE,\n",
    "    metrics=[accuracy],\n",
    "    callback_fns=[partial(TrainMetricsRecorder, show_graph=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfreeze our CNN since we're training all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `fit` function to train the dnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit(EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can plot loss by using the default callback Recorder.\n",
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model, lets take a look at the accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, metric = learn.validate(learn.data.valid_dl, metrics=[accuracy])\n",
    "print(f'Accuracy on validation set: {100*float(metric):3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, analyze the classification results by using `ClassificationInterpretation` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "# Get prediction scores. We convert tensors to numpy array to plot them later.\n",
    "pred_scores = to_np(interp.probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot precision-recall and ROC curves for each class as well. Please note that these plots are not too interesting here, since the dataset is easy and thus the accuracy is close to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels of the validation set. We convert to numpy array for plotting.\n",
    "true_labels = to_np(interp.y_true)\n",
    "plot_pr_roc_curves(true_labels, pred_scores, data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a close look how our model confused some of the samples (if any). The most common way to do that is to use a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating our results, we want to see where the model messes up, and whether or not we can do better. So we're interested in seeing images where the model predicted the image incorrectly but with high confidence (images with the highest loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, figsize=(15,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty much it! Now you can bring your own dataset and train your model on them easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "\n",
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvbp",
   "language": "python",
   "name": "cvbp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
